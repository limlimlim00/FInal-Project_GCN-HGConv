# Comparative Study of GCN and HGConv: Structure, Features, and Noise

This repository contains code and results for the project **"Comparative Study of GCN and HGConv: Structure, Features, and Noise"**, conducted as part of the **Artificial Intelligence** course final project at Seoul National University.

The goal of this project is to systematically compare **Graph Convolutional Network (GCN)** and **Hypergraph Convolution (HGConv)** models under various conditions, focusing on structural generalization and robustness to input perturbations.

## ðŸ“ Repository Structure

```
.
â”œâ”€â”€ Performance/
â”‚   â”œâ”€â”€ gcn_vs_hgconv_accuracy_analysis.ipynb
â”‚   â””â”€â”€ Figures/        # Test accuracy plots on clean datasets
â”œâ”€â”€ Perturbation/
â”‚   â”œâ”€â”€ robustness_generalization_test.ipynb
â”‚   â””â”€â”€ Figures/        # Test accuracy plots under noisy settings (Gaussian noise, edge perturbation, label noise)
â”œâ”€â”€ FinalProject_GCN_vs_HGConv.pdf
â””â”€â”€ README.md
```

Each directory includes:

- A Jupyter notebook (`.ipynb`) that runs the corresponding experiments.
- A `Figures/` folder that stores the resulting plots from both clean and corrupted input settings.

## ðŸ§ª Experiments Overview

We evaluated GCN and HGConv models under two experimental settings:

1. **Clean Setting**  
   - **Datasets**: Cora, Citeseer, Pubmed, BAShapes (with and without node features)  
   - **Hyperedge Construction**: 1-hop neighbors, k-NN (cosine similarity)  
   - **Evaluation Metric**: Test accuracy at the epoch of best validation performance  
   - For BAShapes (with features), node features were synthetically generated by sampling from class-dependent Gaussian distributions with mean shifts proportional to their labels.

2. **Robustness Setting**  
   - **Datasets**: Cora, Citeseer, Pubmed  
   - **Hyperedge Construction**: 1-hop neighbors only  
   - **Evaluation Metric**: Test accuracy at the epoch of best validation performance  
   - **Corruption Types**:
     - **Feature Noise**: Add Gaussian noise (Ïƒ = 0.1) to node features  
     - **Structural Perturbation**: Randomly add or remove 5% of edges  
     - **Label Noise**: Flip 10% of training labels

All models use 2-layer GCN or HGConv architectures implemented in PyTorch Geometric (PyG).

## ðŸ“Š Key Results

| Dataset                  |           Clean Setting        |      Feature Noise Setting     |
|--------------------------|--------------------------------|--------------------------------|
| Cora                     | GCN â‰ˆ HGConv-1hop > HGConv-kNN | HGConv-1hop > GCN (**+14.8%**) |
| Citeseer                 | GCN â‰ˆ HGConv-1hop > HGConv-kNN | HGConv-1hop > GCN (**+8.6%**)  |
| Pubmed                   | GCN â‰ˆ HGConv-1hop > HGConv-kNN | HGConv-1hop > GCN (**+15.1%**) |
| BAShapes                 | GCN > HGConv-1hop â‰ˆ HGConv-kNN | -                              |
| BAShapes (with features) | HGConv-kNN > HGConv-1hop > GCN | -                              |

- **In label-derived feature settings, the choice of hyperedges strongly influences model performance.**
- **HGConv-1hop demonstrates consistently stronger robustness to input corruption compared to GCN across all datasets.**

## ðŸ“Œ Notes

- All experiments were run on NVIDIA A100 GPU (Colab).
- Hyperparameters: Adam optimizer, 200 epochs, hidden dim=16, dropout=0.5
