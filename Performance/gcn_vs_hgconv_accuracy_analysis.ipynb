{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TADCLwg4kOIh",
        "outputId": "9044ab86-ee0a-42ea-de77-d9f2b712697b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_scatter, pyg_lib, torch_sparse\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, BAShapes\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import HypergraphConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "fXe_kZT5-RiK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 마스크 생성 함수\n",
        "def create_scaled_masks(data, train_ratio=0.1, val_ratio=0.15, test_ratio=0.25, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    num_nodes = data.num_nodes\n",
        "    labels = data.y.cpu().numpy()\n",
        "    num_classes = labels.max() + 1\n",
        "\n",
        "    train_per_class = max(1, int(train_ratio * num_nodes / num_classes))\n",
        "    val_total = int(val_ratio * num_nodes)\n",
        "    test_total = int(test_ratio * num_nodes)\n",
        "\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        idx = np.where(labels == c)[0]\n",
        "        chosen = np.random.choice(idx, size=min(train_per_class, len(idx)), replace=False)\n",
        "        train_mask[chosen] = True\n",
        "\n",
        "    rest_idx = np.where(~train_mask.cpu().numpy())[0]\n",
        "    chosen_val = np.random.choice(rest_idx, size=min(val_total, len(rest_idx)), replace=False)\n",
        "    val_mask[chosen_val] = True\n",
        "\n",
        "    rest_idx2 = np.setdiff1d(rest_idx, chosen_val)\n",
        "    chosen_test = np.random.choice(rest_idx2, size=min(test_total, len(rest_idx2)), replace=False)\n",
        "    test_mask[chosen_test] = True\n",
        "\n",
        "    data.train_mask = train_mask\n",
        "    data.val_mask = val_mask\n",
        "    data.test_mask = test_mask\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "XpJKtcdy8Dr4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BAShapes feature vector 생성\n",
        "def create_mean_shifted_features(data, shift_scale=4.0, seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    num_features = data.num_features\n",
        "    num_classes = data.y.max().item() + 1\n",
        "\n",
        "    new_x = torch.zeros(data.num_nodes, num_features, device=data.x.device)\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        class_mask = (data.y == c)\n",
        "        idx = class_mask.nonzero(as_tuple=True)[0]  # 클래스 c에 해당하는 노드 인덱스\n",
        "        n_c = idx.size(0)\n",
        "        x_c = torch.randn(n_c, num_features, device=data.x.device) + c * shift_scale  # 클래스별 평균 다른 정규분포 샘플링\n",
        "        new_x[idx] = x_c\n",
        "\n",
        "    data.x = new_x\n",
        "    return data"
      ],
      "metadata": {
        "id": "K3gsLRwyHOhI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1-hop 하이퍼엣지 생성\n",
        "def generate_1hop_hyperedge_index(data):\n",
        "    edge_index = data.edge_index\n",
        "    num_nodes = data.num_nodes\n",
        "    edge_dict = defaultdict(set)\n",
        "\n",
        "    # 1-hop 이웃 관계 구성 (양방향으로 간주)\n",
        "    for src, tgt in edge_index.t().tolist():\n",
        "        edge_dict[src].add(tgt)\n",
        "        edge_dict[tgt].add(src)\n",
        "\n",
        "    # hyperedge 생성: 각 노드 + 그 이웃들 = 하나의 hyperedge\n",
        "    node_list = []\n",
        "    hyperedge_list = []\n",
        "    for hyperedge_id, node in enumerate(range(num_nodes)):\n",
        "        group = edge_dict[node] | {node}  # 자신 포함\n",
        "        for n in group:\n",
        "            node_list.append(n)\n",
        "            hyperedge_list.append(hyperedge_id)\n",
        "\n",
        "    hyperedge_index = torch.tensor([node_list, hyperedge_list], dtype=torch.long)\n",
        "    return hyperedge_index"
      ],
      "metadata": {
        "id": "8zjWkYkN9qbu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA + k-nn 하이퍼엣지 생성\n",
        "def generate_pca_knn_hyperedge_index(data: Data, k: int, pca_dim: int = 0) -> torch.Tensor:\n",
        "    x = data.x\n",
        "    if x.is_sparse:\n",
        "        x = x.to_dense()\n",
        "    x = x.cpu().numpy()\n",
        "\n",
        "    # 차원이 100 보다 작은 경우 pca_dim을 0으로 주어 PCA 진행 X\n",
        "    if pca_dim > 0:\n",
        "      x_reduced = PCA(n_components=pca_dim).fit_transform(x)\n",
        "    else:\n",
        "      x_reduced = x\n",
        "\n",
        "    sim_matrix = cosine_similarity(x_reduced)\n",
        "\n",
        "    node_list, hyperedge_list = [], []\n",
        "    num_nodes = sim_matrix.shape[0]\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        sim_matrix[i, i] = -1\n",
        "        top_k = sim_matrix[i].argsort()[-(k+1):]\n",
        "        for j in top_k:\n",
        "            node_list.append(j)\n",
        "            hyperedge_list.append(i)\n",
        "\n",
        "    hyperedge_index = torch.tensor([node_list, hyperedge_list], dtype=torch.long)\n",
        "\n",
        "    return hyperedge_index"
      ],
      "metadata": {
        "id": "UXncSnrP0izy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset: Cora, Citeseer, Pubmed, BAShapes\n",
        "\n",
        "normalize = NormalizeFeatures()\n",
        "datasets = {}\n",
        "\n",
        "# Planetoid\n",
        "planetoid_names = ['Cora', 'Citeseer', 'Pubmed']\n",
        "knn_config = {  # 평균 degree 반올림 + 자기자신\n",
        "    'Cora': 4+1,\n",
        "    'Citeseer': 3+1,\n",
        "    'Pubmed': 4+1\n",
        "}\n",
        "\n",
        "for name in planetoid_names:\n",
        "    data = Planetoid(root=f'data/{name}', name=name, transform=normalize)[0]\n",
        "\n",
        "    data_1hop = data.clone()\n",
        "    data_1hop.hyperedge_index = generate_1hop_hyperedge_index(data_1hop)\n",
        "    datasets[f'{name}_1hop'] = data_1hop\n",
        "\n",
        "    data_knn = data.clone()\n",
        "    k = knn_config[name]\n",
        "    data_knn.hyperedge_index = generate_pca_knn_hyperedge_index(data_knn, k=k, pca_dim=100)\n",
        "    datasets[f'{name}_k-nn'] = data_knn\n",
        "\n",
        "\n",
        "# BAShapes - 구조 기반 (feature: all ones)\n",
        "bas_struct = BAShapes()[0]\n",
        "bas_struct = normalize(bas_struct)\n",
        "bas_struct = create_scaled_masks(bas_struct)\n",
        "bas_struct.x = torch.ones(bas_struct.num_nodes, 1)\n",
        "\n",
        "bas_struct_1hop = bas_struct.clone()\n",
        "bas_struct_1hop.hyperedge_index = generate_1hop_hyperedge_index(bas_struct_1hop)\n",
        "datasets['BAShapes_1hop'] = bas_struct_1hop\n",
        "\n",
        "bas_struct_knn = bas_struct.clone()\n",
        "bas_struct_knn.hyperedge_index = generate_pca_knn_hyperedge_index(bas_struct_knn, k=6+1)\n",
        "datasets['BAShapes_k-nn'] = bas_struct_knn\n",
        "\n",
        "\n",
        "# BAShapes - feature-injected\n",
        "bas_feat = BAShapes()[0]\n",
        "bas_feat = normalize(bas_feat)\n",
        "bas_feat = create_scaled_masks(bas_feat)\n",
        "bas_feat = create_mean_shifted_features(bas_feat)\n",
        "\n",
        "bas_feat_1hop = bas_feat.clone()\n",
        "bas_feat_1hop.hyperedge_index = generate_1hop_hyperedge_index(bas_feat_1hop)\n",
        "datasets['BAShapes_features_1hop'] = bas_feat_1hop\n",
        "\n",
        "bas_feat_knn = bas_feat.clone()\n",
        "bas_feat_knn.hyperedge_index = generate_pca_knn_hyperedge_index(bas_feat_knn, k=6+1)\n",
        "datasets['BAShapes_features_k-nn'] = bas_feat_knn"
      ],
      "metadata": {
        "id": "Bd7D7LC6nsru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96400314-2b0d-472d-cce6-c0aeca71b4f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'BAShapes' is deprecated, use 'datasets.ExplainerDataset' in combination with 'datasets.graph_generator.BAGraph' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'BAShapes' is deprecated, use 'datasets.ExplainerDataset' in combination with 'datasets.graph_generator.BAGraph' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hypergraph 정보\n",
        "def summarize_hypergraph(data):\n",
        "    if not hasattr(data, 'hyperedge_index'):\n",
        "        return \"No hyperedge_index\"\n",
        "\n",
        "    he = data.hyperedge_index\n",
        "    num_nodes = data.num_nodes\n",
        "    num_hyperedges = he[1].max().item() + 1 if he.numel() > 0 else 0\n",
        "\n",
        "    # 각 하이퍼엣지가 연결한 노드 수\n",
        "    edge_sizes = Counter(he[1].tolist())\n",
        "    avg_size = sum(edge_sizes.values()) / len(edge_sizes) if edge_sizes else 0\n",
        "\n",
        "    return f\"{num_hyperedges} hyperedges, {avg_size:.2f} avg size\"\n",
        "\n",
        "# dataset 정보\n",
        "for name, data in datasets.items():\n",
        "    print(f\"\\n Dataset: {name}\")\n",
        "    print(f\" - Nodes         : {data.num_nodes}\")\n",
        "    print(f\" - Edges         : {data.num_edges}\")\n",
        "    print(f\" - Features      : {data.num_node_features}\")\n",
        "    print(f\" - Classes       : {data.y.unique().numel()}\")\n",
        "    print(f\" - Hypergraph    : {summarize_hypergraph(data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in6wYLzo-b7t",
        "outputId": "944f6fc6-b0cc-430c-f163-f452ae3f0321"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset: Cora_1hop\n",
            " - Nodes         : 2708\n",
            " - Edges         : 10556\n",
            " - Features      : 1433\n",
            " - Classes       : 7\n",
            " - Hypergraph    : 2708 hyperedges, 4.90 avg size\n",
            "\n",
            " Dataset: Cora_k-nn\n",
            " - Nodes         : 2708\n",
            " - Edges         : 10556\n",
            " - Features      : 1433\n",
            " - Classes       : 7\n",
            " - Hypergraph    : 2708 hyperedges, 6.00 avg size\n",
            "\n",
            " Dataset: Citeseer_1hop\n",
            " - Nodes         : 3327\n",
            " - Edges         : 9104\n",
            " - Features      : 3703\n",
            " - Classes       : 6\n",
            " - Hypergraph    : 3327 hyperedges, 3.74 avg size\n",
            "\n",
            " Dataset: Citeseer_k-nn\n",
            " - Nodes         : 3327\n",
            " - Edges         : 9104\n",
            " - Features      : 3703\n",
            " - Classes       : 6\n",
            " - Hypergraph    : 3327 hyperedges, 5.00 avg size\n",
            "\n",
            " Dataset: Pubmed_1hop\n",
            " - Nodes         : 19717\n",
            " - Edges         : 88648\n",
            " - Features      : 500\n",
            " - Classes       : 3\n",
            " - Hypergraph    : 19717 hyperedges, 5.50 avg size\n",
            "\n",
            " Dataset: Pubmed_k-nn\n",
            " - Nodes         : 19717\n",
            " - Edges         : 88648\n",
            " - Features      : 500\n",
            " - Classes       : 3\n",
            " - Hypergraph    : 19717 hyperedges, 6.00 avg size\n",
            "\n",
            " Dataset: BAShapes_1hop\n",
            " - Nodes         : 700\n",
            " - Edges         : 3936\n",
            " - Features      : 1\n",
            " - Classes       : 4\n",
            " - Hypergraph    : 700 hyperedges, 6.62 avg size\n",
            "\n",
            " Dataset: BAShapes_k-nn\n",
            " - Nodes         : 700\n",
            " - Edges         : 3936\n",
            " - Features      : 1\n",
            " - Classes       : 4\n",
            " - Hypergraph    : 700 hyperedges, 8.00 avg size\n",
            "\n",
            " Dataset: BAShapes_features_1hop\n",
            " - Nodes         : 700\n",
            " - Edges         : 3940\n",
            " - Features      : 10\n",
            " - Classes       : 4\n",
            " - Hypergraph    : 700 hyperedges, 6.63 avg size\n",
            "\n",
            " Dataset: BAShapes_features_k-nn\n",
            " - Nodes         : 700\n",
            " - Edges         : 3940\n",
            " - Features      : 10\n",
            " - Classes       : 4\n",
            " - Hypergraph    : 700 hyperedges, 8.00 avg size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GCN\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=16, num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.convs[-1](x, edge_index)\n",
        "\n",
        "    def get_hidden_embeddings(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=False)\n",
        "        return x\n",
        "\n",
        "# HyperGCN\n",
        "class HyperGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=16, num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.convs = ModuleList()\n",
        "        self.convs.append(HypergraphConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(HypergraphConv(hidden_channels, hidden_channels))\n",
        "        self.convs.append(HypergraphConv(hidden_channels, out_channels))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, hyperedge_index = data.x, data.hyperedge_index\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = F.relu(conv(x, hyperedge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return self.convs[-1](x, hyperedge_index)\n",
        "\n",
        "    def get_hidden_embeddings(self, data):\n",
        "        x, hyperedge_index = data.x, data.hyperedge_index\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = F.relu(conv(x, hyperedge_index))\n",
        "            x = F.dropout(x, p=self.dropout, training=False)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SaKII9UxvK0P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test\n",
        "def train(model, data, optimizer, epoch=None):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch is not None and epoch % 10 == 0:\n",
        "        print(f\"[Epoch {epoch:>3}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    accs = []\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "        pred = out[mask].argmax(dim=1)\n",
        "        acc = (pred == data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs"
      ],
      "metadata": {
        "id": "MHYIrYvNvMCn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training, validation, test\n",
        "def run_experiment(name=\"Cora\", model_class=GCN, epochs=200, lr=0.01, weight_decay=5e-4, verbose=False):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    data = datasets[name].to(device)\n",
        "\n",
        "    num_features = data.num_features\n",
        "    num_classes = data.y.unique().numel()\n",
        "\n",
        "    model = model_class(num_features, num_classes).to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    test_acc_at_best_val = 0.0\n",
        "    final_train_acc = 0.0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, data, optimizer, epoch if verbose else None)\n",
        "        accs = test(model, data)\n",
        "        train_acc, val_acc, test_acc = accs\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            test_acc_at_best_val = test_acc\n",
        "            final_train_acc = train_acc\n",
        "\n",
        "        if verbose and epoch % 10 == 0:\n",
        "            print(f\"[Epoch {epoch:>3}] Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_class.__name__,\n",
        "        \"Dataset\": name,\n",
        "        \"Train Acc @ Best Val\": final_train_acc,\n",
        "        \"Best Val Acc\": best_val_acc,\n",
        "        \"Test Acc @ Best Val\": test_acc_at_best_val\n",
        "    }"
      ],
      "metadata": {
        "id": "RTozpbRzxC_O"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실행\n",
        "def run_all_experiments(model_classes=[GCN, HyperGCN], dataset_dict=datasets, verbose=False):\n",
        "    results = []\n",
        "\n",
        "    for model_cls in model_classes:\n",
        "        for name in dataset_dict.keys():\n",
        "            if model_cls.__name__ == \"GCN\" and '_k-nn' in name:  # GCN은 두 번 실험할 필요 X\n",
        "              continue\n",
        "            print(f\"\\n---------- {model_cls.__name__} | {name} ----------\")\n",
        "            result = run_experiment(name=name, model_class=model_cls, verbose=verbose)\n",
        "            results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# 결과 저장 및 출력\n",
        "df = run_all_experiments(verbose=True)\n",
        "print(\"Final Results:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdrU5j-ix74b",
        "outputId": "72b9fe77-f16f-495d-8653-f841b5c696e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------- GCN | Cora_1hop ----------\n",
            "[Epoch  10] Loss: 1.8656\n",
            "[Epoch  10] Val Acc: 0.3080 | Test Acc: 0.3600\n",
            "[Epoch  20] Loss: 1.7324\n",
            "[Epoch  20] Val Acc: 0.6180 | Test Acc: 0.6260\n",
            "[Epoch  30] Loss: 1.5253\n",
            "[Epoch  30] Val Acc: 0.6620 | Test Acc: 0.6750\n",
            "[Epoch  40] Loss: 1.3173\n",
            "[Epoch  40] Val Acc: 0.7320 | Test Acc: 0.7390\n",
            "[Epoch  50] Loss: 1.0889\n",
            "[Epoch  50] Val Acc: 0.7600 | Test Acc: 0.7690\n",
            "[Epoch  60] Loss: 0.9093\n",
            "[Epoch  60] Val Acc: 0.7740 | Test Acc: 0.7860\n",
            "[Epoch  70] Loss: 0.7308\n",
            "[Epoch  70] Val Acc: 0.7680 | Test Acc: 0.7920\n",
            "[Epoch  80] Loss: 0.6568\n",
            "[Epoch  80] Val Acc: 0.7780 | Test Acc: 0.7990\n",
            "[Epoch  90] Loss: 0.6320\n",
            "[Epoch  90] Val Acc: 0.7840 | Test Acc: 0.8020\n",
            "[Epoch 100] Loss: 0.5222\n",
            "[Epoch 100] Val Acc: 0.7880 | Test Acc: 0.8120\n",
            "[Epoch 110] Loss: 0.4943\n",
            "[Epoch 110] Val Acc: 0.7860 | Test Acc: 0.8170\n",
            "[Epoch 120] Loss: 0.4261\n",
            "[Epoch 120] Val Acc: 0.7840 | Test Acc: 0.8000\n",
            "[Epoch 130] Loss: 0.4046\n",
            "[Epoch 130] Val Acc: 0.7800 | Test Acc: 0.7960\n",
            "[Epoch 140] Loss: 0.3975\n",
            "[Epoch 140] Val Acc: 0.7880 | Test Acc: 0.8100\n",
            "[Epoch 150] Loss: 0.4120\n",
            "[Epoch 150] Val Acc: 0.7900 | Test Acc: 0.8100\n",
            "[Epoch 160] Loss: 0.3415\n",
            "[Epoch 160] Val Acc: 0.7920 | Test Acc: 0.8050\n",
            "[Epoch 170] Loss: 0.3531\n",
            "[Epoch 170] Val Acc: 0.7900 | Test Acc: 0.8100\n",
            "[Epoch 180] Loss: 0.3503\n",
            "[Epoch 180] Val Acc: 0.7920 | Test Acc: 0.8090\n",
            "[Epoch 190] Loss: 0.3502\n",
            "[Epoch 190] Val Acc: 0.7960 | Test Acc: 0.8130\n",
            "[Epoch 200] Loss: 0.3043\n",
            "[Epoch 200] Val Acc: 0.7960 | Test Acc: 0.8120\n",
            "\n",
            "---------- GCN | Citeseer_1hop ----------\n",
            "[Epoch  10] Loss: 1.7192\n",
            "[Epoch  10] Val Acc: 0.3500 | Test Acc: 0.3100\n",
            "[Epoch  20] Loss: 1.5928\n",
            "[Epoch  20] Val Acc: 0.5160 | Test Acc: 0.5050\n",
            "[Epoch  30] Loss: 1.4537\n",
            "[Epoch  30] Val Acc: 0.6140 | Test Acc: 0.5920\n",
            "[Epoch  40] Loss: 1.2986\n",
            "[Epoch  40] Val Acc: 0.6480 | Test Acc: 0.6330\n",
            "[Epoch  50] Loss: 1.1501\n",
            "[Epoch  50] Val Acc: 0.6820 | Test Acc: 0.6770\n",
            "[Epoch  60] Loss: 1.0249\n",
            "[Epoch  60] Val Acc: 0.6920 | Test Acc: 0.6900\n",
            "[Epoch  70] Loss: 0.9381\n",
            "[Epoch  70] Val Acc: 0.6960 | Test Acc: 0.6900\n",
            "[Epoch  80] Loss: 0.8571\n",
            "[Epoch  80] Val Acc: 0.6960 | Test Acc: 0.7020\n",
            "[Epoch  90] Loss: 0.7321\n",
            "[Epoch  90] Val Acc: 0.7140 | Test Acc: 0.7060\n",
            "[Epoch 100] Loss: 0.6782\n",
            "[Epoch 100] Val Acc: 0.7100 | Test Acc: 0.7170\n",
            "[Epoch 110] Loss: 0.6178\n",
            "[Epoch 110] Val Acc: 0.7040 | Test Acc: 0.7150\n",
            "[Epoch 120] Loss: 0.5409\n",
            "[Epoch 120] Val Acc: 0.6980 | Test Acc: 0.7160\n",
            "[Epoch 130] Loss: 0.5691\n",
            "[Epoch 130] Val Acc: 0.6960 | Test Acc: 0.7180\n",
            "[Epoch 140] Loss: 0.5406\n",
            "[Epoch 140] Val Acc: 0.6980 | Test Acc: 0.7120\n",
            "[Epoch 150] Loss: 0.5162\n",
            "[Epoch 150] Val Acc: 0.6960 | Test Acc: 0.7200\n",
            "[Epoch 160] Loss: 0.5117\n",
            "[Epoch 160] Val Acc: 0.6980 | Test Acc: 0.7200\n",
            "[Epoch 170] Loss: 0.4729\n",
            "[Epoch 170] Val Acc: 0.6900 | Test Acc: 0.7170\n",
            "[Epoch 180] Loss: 0.5106\n",
            "[Epoch 180] Val Acc: 0.6880 | Test Acc: 0.7180\n",
            "[Epoch 190] Loss: 0.4099\n",
            "[Epoch 190] Val Acc: 0.7040 | Test Acc: 0.7160\n",
            "[Epoch 200] Loss: 0.4251\n",
            "[Epoch 200] Val Acc: 0.6900 | Test Acc: 0.7130\n",
            "\n",
            "---------- GCN | Pubmed_1hop ----------\n",
            "[Epoch  10] Loss: 1.0255\n",
            "[Epoch  10] Val Acc: 0.7200 | Test Acc: 0.6950\n",
            "[Epoch  20] Loss: 0.8976\n",
            "[Epoch  20] Val Acc: 0.7120 | Test Acc: 0.7020\n",
            "[Epoch  30] Loss: 0.7857\n",
            "[Epoch  30] Val Acc: 0.7420 | Test Acc: 0.7320\n",
            "[Epoch  40] Loss: 0.6162\n",
            "[Epoch  40] Val Acc: 0.7480 | Test Acc: 0.7400\n",
            "[Epoch  50] Loss: 0.5265\n",
            "[Epoch  50] Val Acc: 0.7640 | Test Acc: 0.7630\n",
            "[Epoch  60] Loss: 0.4659\n",
            "[Epoch  60] Val Acc: 0.7720 | Test Acc: 0.7670\n",
            "[Epoch  70] Loss: 0.3673\n",
            "[Epoch  70] Val Acc: 0.7840 | Test Acc: 0.7720\n",
            "[Epoch  80] Loss: 0.3296\n",
            "[Epoch  80] Val Acc: 0.7860 | Test Acc: 0.7770\n",
            "[Epoch  90] Loss: 0.3112\n",
            "[Epoch  90] Val Acc: 0.7900 | Test Acc: 0.7890\n",
            "[Epoch 100] Loss: 0.2778\n",
            "[Epoch 100] Val Acc: 0.7900 | Test Acc: 0.7860\n",
            "[Epoch 110] Loss: 0.2649\n",
            "[Epoch 110] Val Acc: 0.7900 | Test Acc: 0.7820\n",
            "[Epoch 120] Loss: 0.2183\n",
            "[Epoch 120] Val Acc: 0.8020 | Test Acc: 0.7910\n",
            "[Epoch 130] Loss: 0.2143\n",
            "[Epoch 130] Val Acc: 0.8100 | Test Acc: 0.7880\n",
            "[Epoch 140] Loss: 0.2477\n",
            "[Epoch 140] Val Acc: 0.7920 | Test Acc: 0.7860\n",
            "[Epoch 150] Loss: 0.2092\n",
            "[Epoch 150] Val Acc: 0.7920 | Test Acc: 0.7910\n",
            "[Epoch 160] Loss: 0.1634\n",
            "[Epoch 160] Val Acc: 0.7880 | Test Acc: 0.7890\n",
            "[Epoch 170] Loss: 0.2084\n",
            "[Epoch 170] Val Acc: 0.8000 | Test Acc: 0.7930\n",
            "[Epoch 180] Loss: 0.2022\n",
            "[Epoch 180] Val Acc: 0.7960 | Test Acc: 0.7970\n",
            "[Epoch 190] Loss: 0.1670\n",
            "[Epoch 190] Val Acc: 0.8040 | Test Acc: 0.7940\n",
            "[Epoch 200] Loss: 0.1816\n",
            "[Epoch 200] Val Acc: 0.7960 | Test Acc: 0.7930\n",
            "\n",
            "---------- GCN | BAShapes_1hop ----------\n",
            "[Epoch  10] Loss: 1.3783\n",
            "[Epoch  10] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  20] Loss: 1.3635\n",
            "[Epoch  20] Val Acc: 0.2476 | Test Acc: 0.2514\n",
            "[Epoch  30] Loss: 1.3666\n",
            "[Epoch  30] Val Acc: 0.4762 | Test Acc: 0.4400\n",
            "[Epoch  40] Loss: 1.3909\n",
            "[Epoch  40] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  50] Loss: 1.3737\n",
            "[Epoch  50] Val Acc: 0.2952 | Test Acc: 0.2629\n",
            "[Epoch  60] Loss: 1.3610\n",
            "[Epoch  60] Val Acc: 0.2952 | Test Acc: 0.2971\n",
            "[Epoch  70] Loss: 1.3736\n",
            "[Epoch  70] Val Acc: 0.2571 | Test Acc: 0.2400\n",
            "[Epoch  80] Loss: 1.3805\n",
            "[Epoch  80] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  90] Loss: 1.3629\n",
            "[Epoch  90] Val Acc: 0.3238 | Test Acc: 0.3429\n",
            "[Epoch 100] Loss: 1.3810\n",
            "[Epoch 100] Val Acc: 0.3333 | Test Acc: 0.3829\n",
            "[Epoch 110] Loss: 1.3635\n",
            "[Epoch 110] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch 120] Loss: 1.3650\n",
            "[Epoch 120] Val Acc: 0.2286 | Test Acc: 0.4000\n",
            "[Epoch 130] Loss: 1.3349\n",
            "[Epoch 130] Val Acc: 0.3619 | Test Acc: 0.4343\n",
            "[Epoch 140] Loss: 1.3501\n",
            "[Epoch 140] Val Acc: 0.5238 | Test Acc: 0.5371\n",
            "[Epoch 150] Loss: 1.3379\n",
            "[Epoch 150] Val Acc: 0.3238 | Test Acc: 0.3600\n",
            "[Epoch 160] Loss: 1.3438\n",
            "[Epoch 160] Val Acc: 0.6000 | Test Acc: 0.6000\n",
            "[Epoch 170] Loss: 1.3574\n",
            "[Epoch 170] Val Acc: 0.3333 | Test Acc: 0.4057\n",
            "[Epoch 180] Loss: 1.3285\n",
            "[Epoch 180] Val Acc: 0.3810 | Test Acc: 0.4743\n",
            "[Epoch 190] Loss: 1.3261\n",
            "[Epoch 190] Val Acc: 0.6190 | Test Acc: 0.5829\n",
            "[Epoch 200] Loss: 1.3393\n",
            "[Epoch 200] Val Acc: 0.5333 | Test Acc: 0.5029\n",
            "\n",
            "---------- GCN | BAShapes_features_1hop ----------\n",
            "[Epoch  10] Loss: 1.2605\n",
            "[Epoch  10] Val Acc: 0.5048 | Test Acc: 0.5257\n",
            "[Epoch  20] Loss: 1.1810\n",
            "[Epoch  20] Val Acc: 0.6857 | Test Acc: 0.6114\n",
            "[Epoch  30] Loss: 1.0716\n",
            "[Epoch  30] Val Acc: 0.6667 | Test Acc: 0.6114\n",
            "[Epoch  40] Loss: 0.9488\n",
            "[Epoch  40] Val Acc: 0.6762 | Test Acc: 0.5943\n",
            "[Epoch  50] Loss: 0.9156\n",
            "[Epoch  50] Val Acc: 0.7048 | Test Acc: 0.6457\n",
            "[Epoch  60] Loss: 0.9042\n",
            "[Epoch  60] Val Acc: 0.6762 | Test Acc: 0.6629\n",
            "[Epoch  70] Loss: 0.8714\n",
            "[Epoch  70] Val Acc: 0.6857 | Test Acc: 0.6400\n",
            "[Epoch  80] Loss: 0.8374\n",
            "[Epoch  80] Val Acc: 0.6952 | Test Acc: 0.6571\n",
            "[Epoch  90] Loss: 0.8232\n",
            "[Epoch  90] Val Acc: 0.7714 | Test Acc: 0.6800\n",
            "[Epoch 100] Loss: 0.8193\n",
            "[Epoch 100] Val Acc: 0.7714 | Test Acc: 0.7429\n",
            "[Epoch 110] Loss: 0.8332\n",
            "[Epoch 110] Val Acc: 0.8095 | Test Acc: 0.7543\n",
            "[Epoch 120] Loss: 0.7783\n",
            "[Epoch 120] Val Acc: 0.8190 | Test Acc: 0.7086\n",
            "[Epoch 130] Loss: 0.7798\n",
            "[Epoch 130] Val Acc: 0.8286 | Test Acc: 0.6857\n",
            "[Epoch 140] Loss: 0.7908\n",
            "[Epoch 140] Val Acc: 0.7714 | Test Acc: 0.7200\n",
            "[Epoch 150] Loss: 0.7574\n",
            "[Epoch 150] Val Acc: 0.8095 | Test Acc: 0.6914\n",
            "[Epoch 160] Loss: 0.7821\n",
            "[Epoch 160] Val Acc: 0.6286 | Test Acc: 0.6971\n",
            "[Epoch 170] Loss: 0.7764\n",
            "[Epoch 170] Val Acc: 0.6381 | Test Acc: 0.6914\n",
            "[Epoch 180] Loss: 0.7674\n",
            "[Epoch 180] Val Acc: 0.7905 | Test Acc: 0.7600\n",
            "[Epoch 190] Loss: 0.7471\n",
            "[Epoch 190] Val Acc: 0.6857 | Test Acc: 0.6571\n",
            "[Epoch 200] Loss: 0.7310\n",
            "[Epoch 200] Val Acc: 0.7714 | Test Acc: 0.6971\n",
            "\n",
            "---------- HyperGCN | Cora_1hop ----------\n",
            "[Epoch  10] Loss: 1.8693\n",
            "[Epoch  10] Val Acc: 0.6960 | Test Acc: 0.6720\n",
            "[Epoch  20] Loss: 1.7409\n",
            "[Epoch  20] Val Acc: 0.7340 | Test Acc: 0.7000\n",
            "[Epoch  30] Loss: 1.5762\n",
            "[Epoch  30] Val Acc: 0.7620 | Test Acc: 0.7460\n",
            "[Epoch  40] Loss: 1.4009\n",
            "[Epoch  40] Val Acc: 0.7620 | Test Acc: 0.7440\n",
            "[Epoch  50] Loss: 1.1685\n",
            "[Epoch  50] Val Acc: 0.7800 | Test Acc: 0.7840\n",
            "[Epoch  60] Loss: 0.9911\n",
            "[Epoch  60] Val Acc: 0.7940 | Test Acc: 0.7830\n",
            "[Epoch  70] Loss: 0.8634\n",
            "[Epoch  70] Val Acc: 0.7980 | Test Acc: 0.7880\n",
            "[Epoch  80] Loss: 0.7590\n",
            "[Epoch  80] Val Acc: 0.8060 | Test Acc: 0.8040\n",
            "[Epoch  90] Loss: 0.6557\n",
            "[Epoch  90] Val Acc: 0.8020 | Test Acc: 0.8050\n",
            "[Epoch 100] Loss: 0.5899\n",
            "[Epoch 100] Val Acc: 0.8040 | Test Acc: 0.8090\n",
            "[Epoch 110] Loss: 0.5097\n",
            "[Epoch 110] Val Acc: 0.8000 | Test Acc: 0.8100\n",
            "[Epoch 120] Loss: 0.4890\n",
            "[Epoch 120] Val Acc: 0.7980 | Test Acc: 0.8040\n",
            "[Epoch 130] Loss: 0.4337\n",
            "[Epoch 130] Val Acc: 0.8000 | Test Acc: 0.7990\n",
            "[Epoch 140] Loss: 0.4258\n",
            "[Epoch 140] Val Acc: 0.7980 | Test Acc: 0.7990\n",
            "[Epoch 150] Loss: 0.3927\n",
            "[Epoch 150] Val Acc: 0.8000 | Test Acc: 0.8040\n",
            "[Epoch 160] Loss: 0.3688\n",
            "[Epoch 160] Val Acc: 0.7980 | Test Acc: 0.8030\n",
            "[Epoch 170] Loss: 0.3923\n",
            "[Epoch 170] Val Acc: 0.8040 | Test Acc: 0.8180\n",
            "[Epoch 180] Loss: 0.3792\n",
            "[Epoch 180] Val Acc: 0.7980 | Test Acc: 0.8100\n",
            "[Epoch 190] Loss: 0.3516\n",
            "[Epoch 190] Val Acc: 0.8040 | Test Acc: 0.8090\n",
            "[Epoch 200] Loss: 0.3421\n",
            "[Epoch 200] Val Acc: 0.8040 | Test Acc: 0.8060\n",
            "\n",
            "---------- HyperGCN | Cora_k-nn ----------\n",
            "[Epoch  10] Loss: 1.9048\n",
            "[Epoch  10] Val Acc: 0.4300 | Test Acc: 0.4550\n",
            "[Epoch  20] Loss: 1.8386\n",
            "[Epoch  20] Val Acc: 0.4320 | Test Acc: 0.4350\n",
            "[Epoch  30] Loss: 1.7494\n",
            "[Epoch  30] Val Acc: 0.4940 | Test Acc: 0.5180\n",
            "[Epoch  40] Loss: 1.6452\n",
            "[Epoch  40] Val Acc: 0.4720 | Test Acc: 0.4980\n",
            "[Epoch  50] Loss: 1.5500\n",
            "[Epoch  50] Val Acc: 0.5320 | Test Acc: 0.5630\n",
            "[Epoch  60] Loss: 1.4251\n",
            "[Epoch  60] Val Acc: 0.5320 | Test Acc: 0.5450\n",
            "[Epoch  70] Loss: 1.3285\n",
            "[Epoch  70] Val Acc: 0.5560 | Test Acc: 0.5620\n",
            "[Epoch  80] Loss: 1.2314\n",
            "[Epoch  80] Val Acc: 0.5460 | Test Acc: 0.5600\n",
            "[Epoch  90] Loss: 1.1621\n",
            "[Epoch  90] Val Acc: 0.5520 | Test Acc: 0.5570\n",
            "[Epoch 100] Loss: 1.1085\n",
            "[Epoch 100] Val Acc: 0.5460 | Test Acc: 0.5580\n",
            "[Epoch 110] Loss: 1.0465\n",
            "[Epoch 110] Val Acc: 0.5560 | Test Acc: 0.5610\n",
            "[Epoch 120] Loss: 1.0185\n",
            "[Epoch 120] Val Acc: 0.5520 | Test Acc: 0.5580\n",
            "[Epoch 130] Loss: 0.9454\n",
            "[Epoch 130] Val Acc: 0.5500 | Test Acc: 0.5600\n",
            "[Epoch 140] Loss: 0.9217\n",
            "[Epoch 140] Val Acc: 0.5520 | Test Acc: 0.5550\n",
            "[Epoch 150] Loss: 0.8607\n",
            "[Epoch 150] Val Acc: 0.5560 | Test Acc: 0.5610\n",
            "[Epoch 160] Loss: 0.8579\n",
            "[Epoch 160] Val Acc: 0.5480 | Test Acc: 0.5580\n",
            "[Epoch 170] Loss: 0.8419\n",
            "[Epoch 170] Val Acc: 0.5620 | Test Acc: 0.5590\n",
            "[Epoch 180] Loss: 0.8237\n",
            "[Epoch 180] Val Acc: 0.5520 | Test Acc: 0.5680\n",
            "[Epoch 190] Loss: 0.7732\n",
            "[Epoch 190] Val Acc: 0.5560 | Test Acc: 0.5820\n",
            "[Epoch 200] Loss: 0.7475\n",
            "[Epoch 200] Val Acc: 0.5600 | Test Acc: 0.5740\n",
            "\n",
            "---------- HyperGCN | Citeseer_1hop ----------\n",
            "[Epoch  10] Loss: 1.7279\n",
            "[Epoch  10] Val Acc: 0.6660 | Test Acc: 0.6790\n",
            "[Epoch  20] Loss: 1.6332\n",
            "[Epoch  20] Val Acc: 0.7020 | Test Acc: 0.6730\n",
            "[Epoch  30] Loss: 1.4900\n",
            "[Epoch  30] Val Acc: 0.7000 | Test Acc: 0.6970\n",
            "[Epoch  40] Loss: 1.3592\n",
            "[Epoch  40] Val Acc: 0.7120 | Test Acc: 0.6930\n",
            "[Epoch  50] Loss: 1.2082\n",
            "[Epoch  50] Val Acc: 0.7080 | Test Acc: 0.6890\n",
            "[Epoch  60] Loss: 1.0463\n",
            "[Epoch  60] Val Acc: 0.7160 | Test Acc: 0.6960\n",
            "[Epoch  70] Loss: 0.9454\n",
            "[Epoch  70] Val Acc: 0.7100 | Test Acc: 0.7000\n",
            "[Epoch  80] Loss: 0.8500\n",
            "[Epoch  80] Val Acc: 0.7120 | Test Acc: 0.6910\n",
            "[Epoch  90] Loss: 0.7538\n",
            "[Epoch  90] Val Acc: 0.7080 | Test Acc: 0.6970\n",
            "[Epoch 100] Loss: 0.6970\n",
            "[Epoch 100] Val Acc: 0.7080 | Test Acc: 0.7030\n",
            "[Epoch 110] Loss: 0.6484\n",
            "[Epoch 110] Val Acc: 0.7100 | Test Acc: 0.7070\n",
            "[Epoch 120] Loss: 0.6248\n",
            "[Epoch 120] Val Acc: 0.7040 | Test Acc: 0.7000\n",
            "[Epoch 130] Loss: 0.6108\n",
            "[Epoch 130] Val Acc: 0.7060 | Test Acc: 0.7020\n",
            "[Epoch 140] Loss: 0.5734\n",
            "[Epoch 140] Val Acc: 0.7140 | Test Acc: 0.7010\n",
            "[Epoch 150] Loss: 0.5179\n",
            "[Epoch 150] Val Acc: 0.7100 | Test Acc: 0.7070\n",
            "[Epoch 160] Loss: 0.5299\n",
            "[Epoch 160] Val Acc: 0.7080 | Test Acc: 0.7140\n",
            "[Epoch 170] Loss: 0.5022\n",
            "[Epoch 170] Val Acc: 0.7080 | Test Acc: 0.7100\n",
            "[Epoch 180] Loss: 0.4869\n",
            "[Epoch 180] Val Acc: 0.7120 | Test Acc: 0.7100\n",
            "[Epoch 190] Loss: 0.4430\n",
            "[Epoch 190] Val Acc: 0.7140 | Test Acc: 0.7070\n",
            "[Epoch 200] Loss: 0.4386\n",
            "[Epoch 200] Val Acc: 0.7080 | Test Acc: 0.7040\n",
            "\n",
            "---------- HyperGCN | Citeseer_k-nn ----------\n",
            "[Epoch  10] Loss: 1.7650\n",
            "[Epoch  10] Val Acc: 0.5260 | Test Acc: 0.4730\n",
            "[Epoch  20] Loss: 1.7319\n",
            "[Epoch  20] Val Acc: 0.5380 | Test Acc: 0.5420\n",
            "[Epoch  30] Loss: 1.6862\n",
            "[Epoch  30] Val Acc: 0.5940 | Test Acc: 0.5540\n",
            "[Epoch  40] Loss: 1.6325\n",
            "[Epoch  40] Val Acc: 0.5860 | Test Acc: 0.5860\n",
            "[Epoch  50] Loss: 1.5611\n",
            "[Epoch  50] Val Acc: 0.5860 | Test Acc: 0.5720\n",
            "[Epoch  60] Loss: 1.5018\n",
            "[Epoch  60] Val Acc: 0.5780 | Test Acc: 0.5880\n",
            "[Epoch  70] Loss: 1.4285\n",
            "[Epoch  70] Val Acc: 0.5880 | Test Acc: 0.5890\n",
            "[Epoch  80] Loss: 1.3669\n",
            "[Epoch  80] Val Acc: 0.6000 | Test Acc: 0.5950\n",
            "[Epoch  90] Loss: 1.2986\n",
            "[Epoch  90] Val Acc: 0.5920 | Test Acc: 0.6020\n",
            "[Epoch 100] Loss: 1.2485\n",
            "[Epoch 100] Val Acc: 0.5960 | Test Acc: 0.6030\n",
            "[Epoch 110] Loss: 1.1578\n",
            "[Epoch 110] Val Acc: 0.5820 | Test Acc: 0.5880\n",
            "[Epoch 120] Loss: 1.1480\n",
            "[Epoch 120] Val Acc: 0.5860 | Test Acc: 0.5960\n",
            "[Epoch 130] Loss: 1.1312\n",
            "[Epoch 130] Val Acc: 0.5880 | Test Acc: 0.5870\n",
            "[Epoch 140] Loss: 1.0872\n",
            "[Epoch 140] Val Acc: 0.5840 | Test Acc: 0.5910\n",
            "[Epoch 150] Loss: 1.0643\n",
            "[Epoch 150] Val Acc: 0.5860 | Test Acc: 0.5880\n",
            "[Epoch 160] Loss: 1.0104\n",
            "[Epoch 160] Val Acc: 0.5960 | Test Acc: 0.5980\n",
            "[Epoch 170] Loss: 1.0013\n",
            "[Epoch 170] Val Acc: 0.6020 | Test Acc: 0.5900\n",
            "[Epoch 180] Loss: 0.9830\n",
            "[Epoch 180] Val Acc: 0.5940 | Test Acc: 0.5840\n",
            "[Epoch 190] Loss: 0.9797\n",
            "[Epoch 190] Val Acc: 0.5960 | Test Acc: 0.5970\n",
            "[Epoch 200] Loss: 0.9575\n",
            "[Epoch 200] Val Acc: 0.5920 | Test Acc: 0.5950\n",
            "\n",
            "---------- HyperGCN | Pubmed_1hop ----------\n",
            "[Epoch  10] Loss: 1.0090\n",
            "[Epoch  10] Val Acc: 0.7180 | Test Acc: 0.6830\n",
            "[Epoch  20] Loss: 0.8612\n",
            "[Epoch  20] Val Acc: 0.7220 | Test Acc: 0.6900\n",
            "[Epoch  30] Loss: 0.6948\n",
            "[Epoch  30] Val Acc: 0.7240 | Test Acc: 0.6960\n",
            "[Epoch  40] Loss: 0.6265\n",
            "[Epoch  40] Val Acc: 0.7380 | Test Acc: 0.7140\n",
            "[Epoch  50] Loss: 0.5003\n",
            "[Epoch  50] Val Acc: 0.7700 | Test Acc: 0.7370\n",
            "[Epoch  60] Loss: 0.4098\n",
            "[Epoch  60] Val Acc: 0.7860 | Test Acc: 0.7510\n",
            "[Epoch  70] Loss: 0.3529\n",
            "[Epoch  70] Val Acc: 0.7880 | Test Acc: 0.7640\n",
            "[Epoch  80] Loss: 0.3090\n",
            "[Epoch  80] Val Acc: 0.7980 | Test Acc: 0.7650\n",
            "[Epoch  90] Loss: 0.2890\n",
            "[Epoch  90] Val Acc: 0.7980 | Test Acc: 0.7670\n",
            "[Epoch 100] Loss: 0.2359\n",
            "[Epoch 100] Val Acc: 0.7940 | Test Acc: 0.7760\n",
            "[Epoch 110] Loss: 0.2298\n",
            "[Epoch 110] Val Acc: 0.8020 | Test Acc: 0.7810\n",
            "[Epoch 120] Loss: 0.2127\n",
            "[Epoch 120] Val Acc: 0.8020 | Test Acc: 0.7870\n",
            "[Epoch 130] Loss: 0.2354\n",
            "[Epoch 130] Val Acc: 0.8020 | Test Acc: 0.7850\n",
            "[Epoch 140] Loss: 0.1931\n",
            "[Epoch 140] Val Acc: 0.7960 | Test Acc: 0.7790\n",
            "[Epoch 150] Loss: 0.1891\n",
            "[Epoch 150] Val Acc: 0.8000 | Test Acc: 0.7800\n",
            "[Epoch 160] Loss: 0.1802\n",
            "[Epoch 160] Val Acc: 0.8000 | Test Acc: 0.7760\n",
            "[Epoch 170] Loss: 0.1643\n",
            "[Epoch 170] Val Acc: 0.8060 | Test Acc: 0.7770\n",
            "[Epoch 180] Loss: 0.1696\n",
            "[Epoch 180] Val Acc: 0.7980 | Test Acc: 0.7830\n",
            "[Epoch 190] Loss: 0.1551\n",
            "[Epoch 190] Val Acc: 0.8000 | Test Acc: 0.7780\n",
            "[Epoch 200] Loss: 0.1698\n",
            "[Epoch 200] Val Acc: 0.7960 | Test Acc: 0.7800\n",
            "\n",
            "---------- HyperGCN | Pubmed_k-nn ----------\n",
            "[Epoch  10] Loss: 1.0019\n",
            "[Epoch  10] Val Acc: 0.6460 | Test Acc: 0.6470\n",
            "[Epoch  20] Loss: 0.8600\n",
            "[Epoch  20] Val Acc: 0.6760 | Test Acc: 0.6760\n",
            "[Epoch  30] Loss: 0.6703\n",
            "[Epoch  30] Val Acc: 0.6780 | Test Acc: 0.6740\n",
            "[Epoch  40] Loss: 0.5335\n",
            "[Epoch  40] Val Acc: 0.6840 | Test Acc: 0.6800\n",
            "[Epoch  50] Loss: 0.4501\n",
            "[Epoch  50] Val Acc: 0.7060 | Test Acc: 0.6800\n",
            "[Epoch  60] Loss: 0.3747\n",
            "[Epoch  60] Val Acc: 0.7020 | Test Acc: 0.6810\n",
            "[Epoch  70] Loss: 0.3611\n",
            "[Epoch  70] Val Acc: 0.7040 | Test Acc: 0.6840\n",
            "[Epoch  80] Loss: 0.3157\n",
            "[Epoch  80] Val Acc: 0.6960 | Test Acc: 0.6760\n",
            "[Epoch  90] Loss: 0.3194\n",
            "[Epoch  90] Val Acc: 0.6920 | Test Acc: 0.6780\n",
            "[Epoch 100] Loss: 0.3006\n",
            "[Epoch 100] Val Acc: 0.6820 | Test Acc: 0.6760\n",
            "[Epoch 110] Loss: 0.2999\n",
            "[Epoch 110] Val Acc: 0.6860 | Test Acc: 0.6750\n",
            "[Epoch 120] Loss: 0.3034\n",
            "[Epoch 120] Val Acc: 0.6840 | Test Acc: 0.6750\n",
            "[Epoch 130] Loss: 0.2837\n",
            "[Epoch 130] Val Acc: 0.6840 | Test Acc: 0.6740\n",
            "[Epoch 140] Loss: 0.2820\n",
            "[Epoch 140] Val Acc: 0.6860 | Test Acc: 0.6730\n",
            "[Epoch 150] Loss: 0.2692\n",
            "[Epoch 150] Val Acc: 0.6640 | Test Acc: 0.6700\n",
            "[Epoch 160] Loss: 0.2813\n",
            "[Epoch 160] Val Acc: 0.6760 | Test Acc: 0.6700\n",
            "[Epoch 170] Loss: 0.2596\n",
            "[Epoch 170] Val Acc: 0.6760 | Test Acc: 0.6680\n",
            "[Epoch 180] Loss: 0.2647\n",
            "[Epoch 180] Val Acc: 0.6800 | Test Acc: 0.6670\n",
            "[Epoch 190] Loss: 0.2657\n",
            "[Epoch 190] Val Acc: 0.6820 | Test Acc: 0.6690\n",
            "[Epoch 200] Loss: 0.2545\n",
            "[Epoch 200] Val Acc: 0.6760 | Test Acc: 0.6700\n",
            "\n",
            "---------- HyperGCN | BAShapes_1hop ----------\n",
            "[Epoch  10] Loss: 1.3891\n",
            "[Epoch  10] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  20] Loss: 1.3868\n",
            "[Epoch  20] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch  30] Loss: 1.3876\n",
            "[Epoch  30] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "[Epoch  40] Loss: 1.3867\n",
            "[Epoch  40] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  50] Loss: 1.3881\n",
            "[Epoch  50] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch  60] Loss: 1.3861\n",
            "[Epoch  60] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  70] Loss: 1.3834\n",
            "[Epoch  70] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "[Epoch  80] Loss: 1.3875\n",
            "[Epoch  80] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch  90] Loss: 1.3855\n",
            "[Epoch  90] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch 100] Loss: 1.3881\n",
            "[Epoch 100] Val Acc: 0.0667 | Test Acc: 0.1314\n",
            "[Epoch 110] Loss: 1.3841\n",
            "[Epoch 110] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch 120] Loss: 1.3859\n",
            "[Epoch 120] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch 130] Loss: 1.3816\n",
            "[Epoch 130] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch 140] Loss: 1.3796\n",
            "[Epoch 140] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch 150] Loss: 1.3872\n",
            "[Epoch 150] Val Acc: 0.0667 | Test Acc: 0.1314\n",
            "[Epoch 160] Loss: 1.3898\n",
            "[Epoch 160] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch 170] Loss: 1.3833\n",
            "[Epoch 170] Val Acc: 0.0667 | Test Acc: 0.1314\n",
            "[Epoch 180] Loss: 1.3879\n",
            "[Epoch 180] Val Acc: 0.2381 | Test Acc: 0.2229\n",
            "[Epoch 190] Loss: 1.3876\n",
            "[Epoch 190] Val Acc: 0.4571 | Test Acc: 0.4343\n",
            "[Epoch 200] Loss: 1.3859\n",
            "[Epoch 200] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "\n",
            "---------- HyperGCN | BAShapes_k-nn ----------\n",
            "[Epoch  10] Loss: 1.3756\n",
            "[Epoch  10] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch  20] Loss: 1.3701\n",
            "[Epoch  20] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "[Epoch  30] Loss: 1.3679\n",
            "[Epoch  30] Val Acc: 0.4571 | Test Acc: 0.4286\n",
            "[Epoch  40] Loss: 1.3667\n",
            "[Epoch  40] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch  50] Loss: 1.3661\n",
            "[Epoch  50] Val Acc: 0.2476 | Test Acc: 0.2114\n",
            "[Epoch  60] Loss: 1.3656\n",
            "[Epoch  60] Val Acc: 0.2476 | Test Acc: 0.2114\n",
            "[Epoch  70] Loss: 1.3659\n",
            "[Epoch  70] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch  80] Loss: 1.3665\n",
            "[Epoch  80] Val Acc: 0.2476 | Test Acc: 0.2114\n",
            "[Epoch  90] Loss: 1.3658\n",
            "[Epoch  90] Val Acc: 0.2476 | Test Acc: 0.2114\n",
            "[Epoch 100] Loss: 1.3656\n",
            "[Epoch 100] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "[Epoch 110] Loss: 1.3658\n",
            "[Epoch 110] Val Acc: 0.2476 | Test Acc: 0.2114\n",
            "[Epoch 120] Loss: 1.3656\n",
            "[Epoch 120] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch 130] Loss: 1.3657\n",
            "[Epoch 130] Val Acc: 0.4571 | Test Acc: 0.4286\n",
            "[Epoch 140] Loss: 1.3656\n",
            "[Epoch 140] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch 150] Loss: 1.3659\n",
            "[Epoch 150] Val Acc: 0.4571 | Test Acc: 0.4286\n",
            "[Epoch 160] Loss: 1.3657\n",
            "[Epoch 160] Val Acc: 0.4571 | Test Acc: 0.4286\n",
            "[Epoch 170] Loss: 1.3658\n",
            "[Epoch 170] Val Acc: 0.4571 | Test Acc: 0.4286\n",
            "[Epoch 180] Loss: 1.3656\n",
            "[Epoch 180] Val Acc: 0.4476 | Test Acc: 0.4286\n",
            "[Epoch 190] Loss: 1.3659\n",
            "[Epoch 190] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "[Epoch 200] Loss: 1.3660\n",
            "[Epoch 200] Val Acc: 0.2381 | Test Acc: 0.2114\n",
            "\n",
            "---------- HyperGCN | BAShapes_features_1hop ----------\n",
            "[Epoch  10] Loss: 2.4772\n",
            "[Epoch  10] Val Acc: 0.6857 | Test Acc: 0.6171\n",
            "[Epoch  20] Loss: 1.1663\n",
            "[Epoch  20] Val Acc: 0.6952 | Test Acc: 0.6400\n",
            "[Epoch  30] Loss: 0.9988\n",
            "[Epoch  30] Val Acc: 0.6952 | Test Acc: 0.6571\n",
            "[Epoch  40] Loss: 0.9223\n",
            "[Epoch  40] Val Acc: 0.7429 | Test Acc: 0.6857\n",
            "[Epoch  50] Loss: 0.8759\n",
            "[Epoch  50] Val Acc: 0.6952 | Test Acc: 0.6857\n",
            "[Epoch  60] Loss: 0.8540\n",
            "[Epoch  60] Val Acc: 0.6762 | Test Acc: 0.6971\n",
            "[Epoch  70] Loss: 0.8307\n",
            "[Epoch  70] Val Acc: 0.7333 | Test Acc: 0.7371\n",
            "[Epoch  80] Loss: 0.8119\n",
            "[Epoch  80] Val Acc: 0.7714 | Test Acc: 0.6914\n",
            "[Epoch  90] Loss: 0.7876\n",
            "[Epoch  90] Val Acc: 0.7524 | Test Acc: 0.7543\n",
            "[Epoch 100] Loss: 0.8227\n",
            "[Epoch 100] Val Acc: 0.8000 | Test Acc: 0.7714\n",
            "[Epoch 110] Loss: 0.7856\n",
            "[Epoch 110] Val Acc: 0.7905 | Test Acc: 0.7543\n",
            "[Epoch 120] Loss: 0.7759\n",
            "[Epoch 120] Val Acc: 0.7714 | Test Acc: 0.7029\n",
            "[Epoch 130] Loss: 0.7616\n",
            "[Epoch 130] Val Acc: 0.8095 | Test Acc: 0.7429\n",
            "[Epoch 140] Loss: 0.7875\n",
            "[Epoch 140] Val Acc: 0.7905 | Test Acc: 0.7771\n",
            "[Epoch 150] Loss: 0.7716\n",
            "[Epoch 150] Val Acc: 0.7429 | Test Acc: 0.7371\n",
            "[Epoch 160] Loss: 0.7606\n",
            "[Epoch 160] Val Acc: 0.7429 | Test Acc: 0.7486\n",
            "[Epoch 170] Loss: 0.7309\n",
            "[Epoch 170] Val Acc: 0.7048 | Test Acc: 0.7886\n",
            "[Epoch 180] Loss: 0.7223\n",
            "[Epoch 180] Val Acc: 0.7905 | Test Acc: 0.7657\n",
            "[Epoch 190] Loss: 0.7650\n",
            "[Epoch 190] Val Acc: 0.8000 | Test Acc: 0.7714\n",
            "[Epoch 200] Loss: 0.7774\n",
            "[Epoch 200] Val Acc: 0.7429 | Test Acc: 0.7714\n",
            "\n",
            "---------- HyperGCN | BAShapes_features_k-nn ----------\n",
            "[Epoch  10] Loss: 1.2206\n",
            "[Epoch  10] Val Acc: 0.5333 | Test Acc: 0.5543\n",
            "[Epoch  20] Loss: 1.0064\n",
            "[Epoch  20] Val Acc: 0.7048 | Test Acc: 0.6514\n",
            "[Epoch  30] Loss: 0.8967\n",
            "[Epoch  30] Val Acc: 0.5524 | Test Acc: 0.5829\n",
            "[Epoch  40] Loss: 0.8523\n",
            "[Epoch  40] Val Acc: 0.6000 | Test Acc: 0.6000\n",
            "[Epoch  50] Loss: 0.7700\n",
            "[Epoch  50] Val Acc: 0.5714 | Test Acc: 0.5886\n",
            "[Epoch  60] Loss: 0.7843\n",
            "[Epoch  60] Val Acc: 0.6571 | Test Acc: 0.6629\n",
            "[Epoch  70] Loss: 0.7862\n",
            "[Epoch  70] Val Acc: 0.6000 | Test Acc: 0.6000\n",
            "[Epoch  80] Loss: 0.7428\n",
            "[Epoch  80] Val Acc: 0.6190 | Test Acc: 0.6286\n",
            "[Epoch  90] Loss: 0.7260\n",
            "[Epoch  90] Val Acc: 0.6952 | Test Acc: 0.7029\n",
            "[Epoch 100] Loss: 0.7139\n",
            "[Epoch 100] Val Acc: 0.6667 | Test Acc: 0.6857\n",
            "[Epoch 110] Loss: 0.6932\n",
            "[Epoch 110] Val Acc: 0.6667 | Test Acc: 0.6686\n",
            "[Epoch 120] Loss: 0.7232\n",
            "[Epoch 120] Val Acc: 0.7333 | Test Acc: 0.7314\n",
            "[Epoch 130] Loss: 0.6429\n",
            "[Epoch 130] Val Acc: 0.7238 | Test Acc: 0.7200\n",
            "[Epoch 140] Loss: 0.6545\n",
            "[Epoch 140] Val Acc: 0.7238 | Test Acc: 0.7429\n",
            "[Epoch 150] Loss: 0.6446\n",
            "[Epoch 150] Val Acc: 0.7619 | Test Acc: 0.7829\n",
            "[Epoch 160] Loss: 0.6657\n",
            "[Epoch 160] Val Acc: 0.7333 | Test Acc: 0.7714\n",
            "[Epoch 170] Loss: 0.6305\n",
            "[Epoch 170] Val Acc: 0.7810 | Test Acc: 0.7714\n",
            "[Epoch 180] Loss: 0.6656\n",
            "[Epoch 180] Val Acc: 0.7619 | Test Acc: 0.7429\n",
            "[Epoch 190] Loss: 0.6631\n",
            "[Epoch 190] Val Acc: 0.7619 | Test Acc: 0.7771\n",
            "[Epoch 200] Loss: 0.5981\n",
            "[Epoch 200] Val Acc: 0.7905 | Test Acc: 0.7886\n",
            "Final Results:\n",
            "       Model                 Dataset  Train Acc @ Best Val  Best Val Acc  \\\n",
            "0        GCN               Cora_1hop              0.992857      0.800000   \n",
            "1        GCN           Citeseer_1hop              0.958333      0.714000   \n",
            "2        GCN             Pubmed_1hop              1.000000      0.810000   \n",
            "3        GCN           BAShapes_1hop              0.558824      0.647619   \n",
            "4        GCN  BAShapes_features_1hop              0.647059      0.828571   \n",
            "5   HyperGCN               Cora_1hop              0.985714      0.810000   \n",
            "6   HyperGCN               Cora_k-nn              0.828571      0.576000   \n",
            "7   HyperGCN           Citeseer_1hop              0.883333      0.720000   \n",
            "8   HyperGCN           Citeseer_k-nn              0.658333      0.618000   \n",
            "9   HyperGCN             Pubmed_1hop              0.983333      0.810000   \n",
            "10  HyperGCN             Pubmed_k-nn              0.883333      0.708000   \n",
            "11  HyperGCN           BAShapes_1hop              0.250000      0.457143   \n",
            "12  HyperGCN           BAShapes_k-nn              0.264706      0.457143   \n",
            "13  HyperGCN  BAShapes_features_1hop              0.588235      0.809524   \n",
            "14  HyperGCN  BAShapes_features_k-nn              0.779412      0.828571   \n",
            "\n",
            "    Test Acc @ Best Val  \n",
            "0              0.812000  \n",
            "1              0.706000  \n",
            "2              0.788000  \n",
            "3              0.582857  \n",
            "4              0.691429  \n",
            "5              0.814000  \n",
            "6              0.584000  \n",
            "7              0.701000  \n",
            "8              0.587000  \n",
            "9              0.786000  \n",
            "10             0.684000  \n",
            "11             0.434286  \n",
            "12             0.428571  \n",
            "13             0.720000  \n",
            "14             0.777143  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def extract_base_dataset(name):\n",
        "    return name.rpartition('_')[0] if '_' in name else name\n",
        "\n",
        "def extract_edge_type(name):\n",
        "    if '_1hop' in name:\n",
        "        return 'HGConv-1hop'\n",
        "    elif '_k-nn' in name:\n",
        "        return 'HGConv-k-NN'\n",
        "    else:\n",
        "        return 'GCN'\n",
        "\n",
        "def plot_grouped_test_accuracy(df, save_path=\"test_accuracy_plot.png\"):\n",
        "    df = df.copy()\n",
        "    df['DatasetType'] = df['Dataset'].apply(extract_base_dataset)\n",
        "    df['EdgeType'] = df['Dataset'].apply(extract_edge_type)\n",
        "\n",
        "    # GCN 명시적으로 처리\n",
        "    df.loc[df['Model'] == 'GCN', 'EdgeType'] = 'GCN'\n",
        "\n",
        "    # 막대 순서 설정\n",
        "    df['EdgeType'] = pd.Categorical(df['EdgeType'], ['GCN', 'HGConv-1hop', 'HGConv-k-NN'], ordered=True)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    sns.barplot(\n",
        "        data=df,\n",
        "        x=\"DatasetType\",\n",
        "        y=\"Test Acc @ Best Val\",\n",
        "        hue=\"EdgeType\",\n",
        "        dodge=True\n",
        "    )\n",
        "\n",
        "    plt.title(\"Test Accuracy by Dataset\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Dataset\")\n",
        "    plt.legend(title=\"Edge Type\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 저장\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "# 사용 예시\n",
        "plot_grouped_test_accuracy(df, save_path=\"grouped_test_accuracy.png\")"
      ],
      "metadata": {
        "id": "RPFdYdNHEZi6"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}